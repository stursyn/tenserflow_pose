{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import click\n",
    "import tensorflow as tf\n",
    "\n",
    "from hourglass104 import StackedHourglassNetwork\n",
    "from preprocess import Preprocessor\n",
    "\n",
    "IMAGE_SHAPE = (256, 256, 3)\n",
    "HEATMAP_SIZE = (64, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 epochs,\n",
    "                 global_batch_size,\n",
    "                 strategy,\n",
    "                 initial_learning_rate,\n",
    "                 version='0.0.1',\n",
    "                 start_epoch=1,\n",
    "                 tensorboard_dir='./logs'):\n",
    "        self.start_epoch = start_epoch\n",
    "        self.model = model\n",
    "        self.epochs = epochs\n",
    "        self.strategy = strategy\n",
    "        self.global_batch_size = global_batch_size\n",
    "        self.loss_object = tf.keras.losses.MeanSquaredError(\n",
    "            reduction=tf.keras.losses.Reduction.NONE)\n",
    "        # \"we use rmsprop with a learning rate of 2.5e-4.\"\"\n",
    "        self.optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=initial_learning_rate)\n",
    "        self.model = model\n",
    "\n",
    "        self.current_learning_rate = initial_learning_rate\n",
    "        self.last_val_loss = math.inf\n",
    "        self.lowest_val_loss = math.inf\n",
    "        self.patience_count = 0\n",
    "        self.max_patience = 10\n",
    "        self.tensorboard_dir = tensorboard_dir\n",
    "        self.best_model = None\n",
    "        self.version = version\n",
    "\n",
    "    def lr_decay(self):\n",
    "        \"\"\"\n",
    "        This effectively simulate ReduceOnPlateau learning rate schedule. Learning rate\n",
    "        will be reduced by a factor of 5 if there's no improvement over [max_patience] epochs\n",
    "        \"\"\"\n",
    "        if self.patience_count >= self.max_patience:\n",
    "            self.current_learning_rate /= 10.0\n",
    "            self.patience_count = 0\n",
    "        elif self.last_val_loss == self.lowest_val_loss:\n",
    "            self.patience_count = 0\n",
    "        self.patience_count += 1\n",
    "\n",
    "        self.optimizer.learning_rate = self.current_learning_rate\n",
    "\n",
    "    def lr_decay_step(self, epoch):\n",
    "        if epoch == 25 or epoch == 50 or epoch == 75:\n",
    "            self.current_learning_rate /= 10.0\n",
    "        self.optimizer.learning_rate = self.current_learning_rate\n",
    "\n",
    "    def compute_loss(self, labels, outputs):\n",
    "        loss = 0\n",
    "        for output in outputs:\n",
    "            # assign more weights to foreground pixels\n",
    "            weights = tf.cast(labels > 0, dtype=tf.float32) * 81 + 1\n",
    "            # loss += tf.reduce_mean(self.loss_object(\n",
    "            #    labels, output)) * (1. / self.global_batch_size)\n",
    "            # loss += tf.math.reduce_sum(tf.math.reduce_mean(tf.math.square(labels - output) * weights, axis=[0,1,2])) * (1. / self.global_batch_size)\n",
    "            loss += tf.math.reduce_mean(\n",
    "                tf.math.square(labels - output) * weights) * (\n",
    "                    1. / self.global_batch_size)\n",
    "        return loss\n",
    "\n",
    "    def train_step(self, inputs):\n",
    "        images, labels = inputs\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = self.model(images, training=True)\n",
    "            loss = self.compute_loss(labels, outputs)\n",
    "\n",
    "        grads = tape.gradient(\n",
    "            target=loss, sources=self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(grads, self.model.trainable_variables))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def val_step(self, inputs):\n",
    "        images, labels = inputs\n",
    "        outputs = self.model(images, training=False)\n",
    "        loss = self.compute_loss(labels, outputs)\n",
    "        return loss\n",
    "\n",
    "    def run(self, train_dist_dataset, val_dist_dataset):\n",
    "        @tf.function\n",
    "        def distributed_train_epoch(dataset):\n",
    "            tf.print('Start distributed traininng...')\n",
    "            total_loss = 0.0\n",
    "            num_train_batches = 0.0\n",
    "            for one_batch in dataset:\n",
    "                per_replica_loss = self.strategy.experimental_run_v2(\n",
    "                    self.train_step, args=(one_batch, ))\n",
    "                batch_loss = self.strategy.reduce(\n",
    "                    tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n",
    "                total_loss += batch_loss\n",
    "                num_train_batches += 1\n",
    "                tf.print('Trained batch', num_train_batches, 'batch loss',\n",
    "                         batch_loss, 'epoch total loss', total_loss)\n",
    "            return total_loss, num_train_batches\n",
    "\n",
    "        @tf.function\n",
    "        def distributed_val_epoch(dataset):\n",
    "            total_loss = 0.0\n",
    "            num_val_batches = 0.0\n",
    "            for one_batch in dataset:\n",
    "                per_replica_loss = self.strategy.experimental_run_v2(\n",
    "                    self.val_step, args=(one_batch, ))\n",
    "                num_val_batches += 1\n",
    "                batch_loss = self.strategy.reduce(\n",
    "                    tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n",
    "                tf.print('Validated batch', num_val_batches, 'batch loss',\n",
    "                         batch_loss)\n",
    "                if not tf.math.is_nan(batch_loss):\n",
    "                    # TODO: Find out why the last validation batch loss become NaN\n",
    "                    total_loss += batch_loss\n",
    "                else:\n",
    "                    num_val_batches -= 1\n",
    "\n",
    "            return total_loss, num_val_batches\n",
    "\n",
    "        summary_writer = tf.summary.create_file_writer(self.tensorboard_dir)\n",
    "        summary_writer.set_as_default()\n",
    "\n",
    "        for epoch in range(self.start_epoch, self.epochs + 1):\n",
    "            tf.summary.experimental.set_step(epoch)\n",
    "\n",
    "            self.lr_decay()\n",
    "            tf.summary.scalar('epoch learning rate',\n",
    "                              self.current_learning_rate)\n",
    "\n",
    "            print('Start epoch {} with learning rate {}'.format(\n",
    "                epoch, self.current_learning_rate))\n",
    "\n",
    "            train_total_loss, num_train_batches = distributed_train_epoch(\n",
    "                train_dist_dataset)\n",
    "            train_loss = train_total_loss / num_train_batches\n",
    "            print('Epoch {} train loss {}'.format(epoch, train_loss))\n",
    "            tf.summary.scalar('epoch train loss', train_loss)\n",
    "\n",
    "            val_total_loss, num_val_batches = distributed_val_epoch(\n",
    "                val_dist_dataset)\n",
    "            val_loss = val_total_loss / num_val_batches\n",
    "            print('Epoch {} val loss {}'.format(epoch, val_loss))\n",
    "            tf.summary.scalar('epoch val loss', val_loss)\n",
    "\n",
    "            # save model when reach a new lowest validation loss\n",
    "            if val_loss < self.lowest_val_loss:\n",
    "                self.save_model(epoch, val_loss)\n",
    "                self.lowest_val_loss = val_loss\n",
    "            self.last_val_loss = val_loss\n",
    "\n",
    "        return self.best_model\n",
    "\n",
    "    def save_model(self, epoch, loss):\n",
    "        model_name = './models/model-v{}-epoch-{}-loss-{:.4f}.h5'.format(\n",
    "            self.version, epoch, loss)\n",
    "        self.model.save_weights(model_name)\n",
    "        self.best_model = model_name\n",
    "        print(\"Model {} saved.\".format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(tfrecords, batch_size, num_heatmap, is_train):\n",
    "    preprocess = Preprocessor(\n",
    "        IMAGE_SHAPE, (HEATMAP_SIZE[0], HEATMAP_SIZE[1], num_heatmap), is_train)\n",
    "\n",
    "    dataset = tf.data.Dataset.list_files(tfrecords)\n",
    "    dataset = tf.data.TFRecordDataset(dataset)\n",
    "    dataset = dataset.map(\n",
    "        preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    if is_train:\n",
    "        dataset = dataset.shuffle(batch_size)\n",
    "\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, start_epoch, learning_rate, tensorboard_dir, checkpoint,\n",
    "          num_heatmap, batch_size, train_tfrecords, val_tfrecords, version):\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    global_batch_size = strategy.num_replicas_in_sync * batch_size\n",
    "    train_dataset = create_dataset(\n",
    "        train_tfrecords, global_batch_size, num_heatmap, is_train=True)\n",
    "    val_dataset = create_dataset(\n",
    "        val_tfrecords, global_batch_size, num_heatmap, is_train=False)\n",
    "\n",
    "    if not os.path.exists(os.path.join('./models')):\n",
    "        os.makedirs(os.path.join('./models/'))\n",
    "\n",
    "    with strategy.scope():\n",
    "        train_dist_dataset = strategy.experimental_distribute_dataset(\n",
    "            train_dataset)\n",
    "        val_dist_dataset = strategy.experimental_distribute_dataset(\n",
    "            val_dataset)\n",
    "\n",
    "        model = StackedHourglassNetwork(IMAGE_SHAPE, 4, 1, num_heatmap)\n",
    "        if checkpoint and os.path.exists(checkpoint):\n",
    "            model.load_weights(checkpoint)\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model,\n",
    "            epochs,\n",
    "            global_batch_size,\n",
    "            strategy,\n",
    "            initial_learning_rate=learning_rate,\n",
    "            start_epoch=start_epoch,\n",
    "            version=version,\n",
    "            tensorboard_dir=tensorboard_dir)\n",
    "\n",
    "        print('Start training...')\n",
    "        return trainer.run(train_dist_dataset, val_dist_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There is non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "Start training...\n",
      "Start epoch 1 with learning rate 0.0001\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 1.33575118 epoch total loss 1.33575118\n",
      "Trained batch 2 batch loss 1.41694987 epoch total loss 2.75270104\n",
      "Trained batch 3 batch loss 1.4375844 epoch total loss 4.19028568\n",
      "Trained batch 4 batch loss 1.47636402 epoch total loss 5.66665\n",
      "Trained batch 5 batch loss 1.43395936 epoch total loss 7.1006093\n",
      "Trained batch 6 batch loss 1.39840293 epoch total loss 8.49901199\n",
      "Trained batch 7 batch loss 1.36231422 epoch total loss 9.86132622\n",
      "Trained batch 8 batch loss 1.35274553 epoch total loss 11.2140713\n",
      "Trained batch 9 batch loss 1.34227312 epoch total loss 12.556344\n",
      "Trained batch 10 batch loss 1.2975322 epoch total loss 13.8538761\n",
      "Trained batch 11 batch loss 1.31212473 epoch total loss 15.166\n",
      "Trained batch 12 batch loss 1.2040689 epoch total loss 16.3700695\n",
      "Trained batch 13 batch loss 1.33911657 epoch total loss 17.7091866\n",
      "Trained batch 14 batch loss 1.18587041 epoch total loss 18.8950577\n",
      "Trained batch 15 batch loss 1.21069396 epoch total loss 20.105751\n",
      "Trained batch 16 batch loss 1.3024019 epoch total loss 21.4081535\n",
      "Trained batch 17 batch loss 1.22215104 epoch total loss 22.6303043\n",
      "Trained batch 18 batch loss 1.25769567 epoch total loss 23.888\n",
      "Trained batch 19 batch loss 1.26736712 epoch total loss 25.1553669\n",
      "Trained batch 20 batch loss 1.2995615 epoch total loss 26.4549294\n",
      "Trained batch 21 batch loss 1.25477314 epoch total loss 27.7097015\n",
      "Trained batch 22 batch loss 1.16363978 epoch total loss 28.8733406\n",
      "Trained batch 23 batch loss 1.16589522 epoch total loss 30.0392361\n",
      "Trained batch 24 batch loss 1.20377123 epoch total loss 31.2430077\n",
      "Trained batch 25 batch loss 1.24698842 epoch total loss 32.4899979\n",
      "Trained batch 26 batch loss 1.15511191 epoch total loss 33.6451111\n",
      "Trained batch 27 batch loss 1.12477934 epoch total loss 34.7698898\n",
      "Trained batch 28 batch loss 1.16815162 epoch total loss 35.9380417\n",
      "Trained batch 29 batch loss 1.13306487 epoch total loss 37.071106\n",
      "Trained batch 30 batch loss 1.17004085 epoch total loss 38.2411461\n",
      "Trained batch 31 batch loss 1.1716342 epoch total loss 39.4127808\n",
      "Trained batch 32 batch loss 1.13158083 epoch total loss 40.5443611\n",
      "Trained batch 33 batch loss 1.14823151 epoch total loss 41.6925926\n",
      "Trained batch 34 batch loss 1.19305241 epoch total loss 42.8856468\n",
      "Trained batch 35 batch loss 1.00209129 epoch total loss 43.8877373\n",
      "Trained batch 36 batch loss 0.99113971 epoch total loss 44.8788757\n",
      "Trained batch 37 batch loss 0.967483401 epoch total loss 45.8463593\n",
      "Trained batch 38 batch loss 0.954639435 epoch total loss 46.801\n",
      "Trained batch 39 batch loss 1.00797367 epoch total loss 47.8089714\n",
      "Trained batch 40 batch loss 1.03482318 epoch total loss 48.8437958\n",
      "Trained batch 41 batch loss 1.07164 epoch total loss 49.9154358\n",
      "Trained batch 42 batch loss 1.05593407 epoch total loss 50.9713707\n",
      "Trained batch 43 batch loss 1.03642094 epoch total loss 52.0077934\n",
      "Trained batch 44 batch loss 1.02756929 epoch total loss 53.0353622\n",
      "Trained batch 45 batch loss 0.957513809 epoch total loss 53.9928741\n",
      "Trained batch 46 batch loss 0.978516698 epoch total loss 54.9713898\n",
      "Trained batch 47 batch loss 0.967816889 epoch total loss 55.9392052\n",
      "Trained batch 48 batch loss 0.926640272 epoch total loss 56.8658447\n",
      "Trained batch 49 batch loss 0.991631866 epoch total loss 57.8574753\n",
      "Trained batch 50 batch loss 0.942329466 epoch total loss 58.7998047\n",
      "Trained batch 51 batch loss 0.935905159 epoch total loss 59.7357101\n",
      "Trained batch 52 batch loss 0.977176309 epoch total loss 60.7128868\n",
      "Trained batch 53 batch loss 0.935738 epoch total loss 61.6486244\n",
      "Trained batch 54 batch loss 0.930859089 epoch total loss 62.579483\n",
      "Trained batch 55 batch loss 0.893343806 epoch total loss 63.4728279\n",
      "Trained batch 56 batch loss 0.90426 epoch total loss 64.3770905\n",
      "Trained batch 57 batch loss 0.854157805 epoch total loss 65.2312469\n",
      "Trained batch 58 batch loss 0.810081065 epoch total loss 66.0413284\n",
      "Trained batch 59 batch loss 0.868665159 epoch total loss 66.909996\n",
      "Trained batch 60 batch loss 0.775491238 epoch total loss 67.6854858\n",
      "Trained batch 61 batch loss 0.784931064 epoch total loss 68.4704132\n",
      "Trained batch 62 batch loss 0.834866643 epoch total loss 69.3052826\n",
      "Trained batch 63 batch loss 0.814746499 epoch total loss 70.1200256\n",
      "Trained batch 64 batch loss 0.848675549 epoch total loss 70.9687042\n",
      "Trained batch 65 batch loss 0.79596585 epoch total loss 71.7646713\n",
      "Trained batch 66 batch loss 0.802709222 epoch total loss 72.5673828\n",
      "Trained batch 67 batch loss 0.828894377 epoch total loss 73.3962784\n",
      "Trained batch 68 batch loss 0.866837144 epoch total loss 74.2631149\n",
      "Trained batch 69 batch loss 0.812054873 epoch total loss 75.0751724\n",
      "Trained batch 70 batch loss 0.650270641 epoch total loss 75.725441\n",
      "Trained batch 71 batch loss 0.740014732 epoch total loss 76.4654541\n",
      "Trained batch 72 batch loss 0.809298754 epoch total loss 77.2747498\n",
      "Trained batch 73 batch loss 0.77381295 epoch total loss 78.0485611\n",
      "Trained batch 74 batch loss 0.788917303 epoch total loss 78.8374786\n",
      "Trained batch 75 batch loss 0.799219 epoch total loss 79.6366959\n",
      "Trained batch 76 batch loss 0.792019248 epoch total loss 80.4287186\n",
      "Trained batch 77 batch loss 0.793119192 epoch total loss 81.2218399\n",
      "Trained batch 78 batch loss 0.786043644 epoch total loss 82.0078812\n",
      "Trained batch 79 batch loss 0.763045192 epoch total loss 82.7709274\n",
      "Trained batch 80 batch loss 0.729435563 epoch total loss 83.5003662\n",
      "Trained batch 81 batch loss 0.727069616 epoch total loss 84.2274323\n",
      "Trained batch 82 batch loss 0.699943542 epoch total loss 84.9273758\n",
      "Trained batch 83 batch loss 0.692968845 epoch total loss 85.6203461\n",
      "Trained batch 84 batch loss 0.719946623 epoch total loss 86.3402939\n",
      "Trained batch 85 batch loss 0.696064949 epoch total loss 87.0363617\n",
      "Trained batch 86 batch loss 0.671425164 epoch total loss 87.7077866\n",
      "Trained batch 87 batch loss 0.726251245 epoch total loss 88.4340363\n",
      "Trained batch 88 batch loss 0.682841301 epoch total loss 89.1168747\n",
      "Trained batch 89 batch loss 0.670077562 epoch total loss 89.7869492\n",
      "Trained batch 90 batch loss 0.689064443 epoch total loss 90.4760132\n",
      "Trained batch 91 batch loss 0.675847411 epoch total loss 91.1518631\n",
      "Trained batch 92 batch loss 0.671028852 epoch total loss 91.8228912\n",
      "Trained batch 93 batch loss 0.676974595 epoch total loss 92.4998627\n",
      "Trained batch 94 batch loss 0.687128663 epoch total loss 93.1869888\n",
      "Trained batch 95 batch loss 0.535165906 epoch total loss 93.7221527\n",
      "Trained batch 96 batch loss 0.546624839 epoch total loss 94.2687759\n",
      "Trained batch 97 batch loss 0.538538337 epoch total loss 94.807312\n",
      "Trained batch 98 batch loss 0.605405807 epoch total loss 95.4127197\n",
      "Trained batch 99 batch loss 0.657354116 epoch total loss 96.070076\n",
      "Trained batch 100 batch loss 0.663924038 epoch total loss 96.734\n",
      "Trained batch 101 batch loss 0.602569 epoch total loss 97.3365707\n",
      "Trained batch 102 batch loss 0.619190454 epoch total loss 97.9557648\n",
      "Trained batch 103 batch loss 0.627635419 epoch total loss 98.5834\n",
      "Trained batch 104 batch loss 0.595371783 epoch total loss 99.178772\n",
      "Trained batch 105 batch loss 0.564312696 epoch total loss 99.7430878\n",
      "Trained batch 106 batch loss 0.616734564 epoch total loss 100.359825\n",
      "Trained batch 107 batch loss 0.535048 epoch total loss 100.894875\n",
      "Trained batch 108 batch loss 0.599105477 epoch total loss 101.49398\n",
      "Trained batch 109 batch loss 0.461855084 epoch total loss 101.955833\n",
      "Trained batch 110 batch loss 0.548794 epoch total loss 102.504631\n",
      "Trained batch 111 batch loss 0.58605814 epoch total loss 103.090691\n",
      "Trained batch 112 batch loss 0.589688599 epoch total loss 103.680382\n",
      "Trained batch 113 batch loss 0.609944582 epoch total loss 104.290329\n",
      "Trained batch 114 batch loss 0.611760199 epoch total loss 104.902092\n",
      "Trained batch 115 batch loss 0.61700964 epoch total loss 105.519104\n",
      "Trained batch 116 batch loss 0.614012361 epoch total loss 106.133118\n",
      "Trained batch 117 batch loss 0.609701276 epoch total loss 106.742821\n",
      "Trained batch 118 batch loss 0.603014529 epoch total loss 107.345833\n",
      "Trained batch 119 batch loss 0.586744487 epoch total loss 107.932579\n",
      "Trained batch 120 batch loss 0.571677446 epoch total loss 108.504257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 121 batch loss 0.593134165 epoch total loss 109.097389\n",
      "Trained batch 122 batch loss 0.51929903 epoch total loss 109.616692\n",
      "Trained batch 123 batch loss 0.552285194 epoch total loss 110.168976\n",
      "Trained batch 124 batch loss 0.568429887 epoch total loss 110.737404\n"
     ]
    }
   ],
   "source": [
    "tfrecords_dir = './dataset/tfrecords_mpii/'\n",
    "train_tfrecords = os.path.join(tfrecords_dir, 'train*')\n",
    "val_tfrecords = os.path.join(tfrecords_dir, 'val*')\n",
    "batch_size = 16\n",
    "num_heatmap = 16\n",
    "tensorboard_dir = './logs/'\n",
    "learning_rate = 0.0001\n",
    "start_epoch = 1\n",
    "epochs = 2\n",
    "\n",
    "train(epochs, start_epoch, learning_rate, tensorboard_dir, None,\n",
    "      num_heatmap, batch_size, train_tfrecords, val_tfrecords, '0.0.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
